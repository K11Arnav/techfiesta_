# -*- coding: utf-8 -*-
"""ftfd_withIso.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i4KDYlV1aQdvxBsCXaOlJskJSguY5MOo
"""

# -*- coding: utf-8 -*-
"""Integrated_Pipeline.ipynb"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import shap
import kagglehub
import os

from sklearn.model_selection import (
    train_test_split,
    StratifiedKFold,
    GridSearchCV,
    cross_val_predict
)
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
from xgboost import XGBClassifier
from sklearn.metrics import (
    classification_report,
    f1_score,
    precision_score,
    recall_score,
    accuracy_score,
    roc_auc_score,
    average_precision_score,
    confusion_matrix,
    precision_recall_curve,
    ConfusionMatrixDisplay
)

# =====================================================================
#                   WRAP ENTIRE PIPELINE IN ONE FUNCTION
# =====================================================================

def run_full_ml_pipeline():
    # 1. Download dataset from Kaggle Hub
    print("ðŸ“¥ Downloading dataset from Kaggle Hub...")
    path = kagglehub.dataset_download("mlg-ulb/creditcardfraud")
    print(f"âœ… Dataset downloaded to: {path}")

    csv_files = [f for f in os.listdir(path) if f.endswith('.csv')]
    print(f"ðŸ“ Available CSV files: {csv_files}")

    df = pd.read_csv(os.path.join(path, "creditcard.csv"))
    print(f"âœ… Dataset loaded successfully!")
    print(f"ðŸ“Š Shape: {df.shape}")

    print("\nMissing Values:\n", df.isnull().sum())

    # 2. Split dataset FIRST (Stratified)
    X = df.drop('Class', axis=1)
    y = df['Class']

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # =================================================================
    #                       XGBOOST SECTION
    # =================================================================
    print("\n" + "="*40)
    print("       STARTING XGBOOST TRAINING")
    print("="*40)

    # Scale Data for XGB
    xgb_scaler = StandardScaler()
    X_train_xgb = X_train.copy()
    X_test_xgb = X_test.copy()

    # Fit on Train, Transform Test
    X_train_xgb[['Amount', 'Time']] = xgb_scaler.fit_transform(X_train[['Amount', 'Time']])
    X_test_xgb[['Amount', 'Time']] = xgb_scaler.transform(X_test[['Amount', 'Time']])

    # Calculate Fraud Ratio for weighting
    fraud_ratio = (y_train == 0).sum() / (y_train == 1).sum()

    # Define Model
    xgb_model = XGBClassifier(
        max_depth=6,
        n_estimators=400,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.8,
        scale_pos_weight=fraud_ratio,
        eval_metric='aucpr',
        tree_method='hist',
        random_state=42,
        n_jobs=-1
    )

    # Fit Model
    xgb_model.fit(X_train_xgb, y_train)

    # --- XGB Threshold Tuning (CV on Train) ---
    y_train_prob_cv = cross_val_predict(
        xgb_model,
        X_train_xgb,
        y_train,
        cv=3,
        method="predict_proba",
        n_jobs=1
    )[:, 1]

    precision, recall, thresholds = precision_recall_curve(y_train, y_train_prob_cv)
    f1_scores = 2 * (precision[:-1] * recall[:-1]) / (precision[:-1] + recall[:-1] + 1e-8)
    best_xgb_threshold = thresholds[np.argmax(f1_scores)]
    print(f"Optimal Threshold (XGB tuned on Train): {best_xgb_threshold:.4f}")

    # Evaluate XGB on Test
    y_test_prob_xgb = xgb_model.predict_proba(X_test_xgb)[:, 1]
    y_test_pred_xgb = (y_test_prob_xgb >= best_xgb_threshold).astype(int)

    print("\nClassification Report (XGB - Test Data):")
    print(classification_report(y_test, y_test_pred_xgb))
    print("PR-AUC:", average_precision_score(y_test, y_test_prob_xgb))

    # SHAP Setup
    explainer = shap.TreeExplainer(xgb_model)
    # (Optional: generate summary plot here if needed, omitted to keep output clean)

    # =================================================================
    #                   ISOLATION FOREST SECTION
    # =================================================================
    print("\n" + "="*40)
    print("    STARTING ISOLATION FOREST TRAINING")
    print("="*40)

    # 1. Feature Engineering (Specific to ISO)
    # Create independent copies
    X_train_iso = X_train.copy()
    X_test_iso = X_test.copy()

    # Convert 'Time' to 'Hour'
    X_train_iso['Hour'] = X_train_iso['Time'].apply(lambda x: np.floor(x / 3600) % 24)
    X_test_iso['Hour'] = X_test_iso['Time'].apply(lambda x: np.floor(x / 3600) % 24)

    # Drop 'Time'
    X_train_iso = X_train_iso.drop('Time', axis=1)
    X_test_iso = X_test_iso.drop('Time', axis=1)

    # 2. Scaling (NO LEAKAGE)
    iso_scaler = StandardScaler()
    cols_to_scale = ['Amount', 'Hour']

    # Fit ONLY on training data
    X_train_iso[cols_to_scale] = iso_scaler.fit_transform(X_train_iso[cols_to_scale])
    # Transform test data
    X_test_iso[cols_to_scale] = iso_scaler.transform(X_test_iso[cols_to_scale])

    # 3. Calculate Contamination
    contamination = (y_train == 1).sum() / len(y_train)
    print(f"Contamination (fraud ratio in training): {contamination:.4f}")

    # 4. Model Training
    iso_model = IsolationForest(
        n_estimators=100,
        max_samples='auto',
        contamination=contamination,
        random_state=42,
        n_jobs=-1
    )

    # Fit on Training Data ONLY
    iso_model.fit(X_train_iso)

    # 5. Threshold Optimization on TRAINING DATA (Snippet 1 Logic)
    # Get anomaly scores on TRAINING set
    y_train_scores = iso_model.decision_function(X_train_iso)
    y_train_prob = -y_train_scores  # Higher score = higher fraud probability

    # Calculate PR Curve
    precision_iso, recall_iso, thresholds_iso = precision_recall_curve(y_train, y_train_prob)

    # Calculate F1 for all thresholds
    numerator = 2 * recall_iso * precision_iso
    denominator = recall_iso + precision_iso
    f1_scores_iso = np.divide(numerator, denominator, out=np.zeros_like(numerator), where=denominator!=0)

    # Find Best Threshold
    best_f1_index = np.argmax(f1_scores_iso)
    best_iso_threshold = thresholds_iso[best_f1_index]
    best_f1_train = f1_scores_iso[best_f1_index]

    print(f"Optimal ISO Threshold (tuned on Train): {best_iso_threshold:.4f}")
    print(f"F1 Score on Training Data: {best_f1_train:.4f}")

    # 6. Final Evaluation on TEST SET
    # Get scores on TEST set
    y_test_scores_iso = iso_model.decision_function(X_test_iso)
    y_test_prob_iso = -y_test_scores_iso

    # Apply learned threshold
    y_test_pred_iso = (y_test_prob_iso >= best_iso_threshold).astype(int)

    print("\nISOLATION FOREST RESULTS (Test Set - No Leakage)")
    print("-" * 40)
    print(classification_report(y_test, y_test_pred_iso))

    cm = confusion_matrix(y_test, y_test_pred_iso)
    print(f"Confusion Matrix:\n{cm}")
    print(f"ROC-AUC:   {roc_auc_score(y_test, y_test_prob_iso):.4f}")
    print(f"PR-AUC:    {average_precision_score(y_test, y_test_prob_iso):.4f}")

    # 7. Prepare Components for Inference Function
    # We need min/max scores for normalization in 'compute_risk_score'
    iso_components = {
        "model": iso_model,
        "scaler": iso_scaler,
        "score_min": y_train_scores.min(),
        "score_max": y_train_scores.max()
    }

    # Return everything needed
    return xgb_model, xgb_scaler, explainer, X_test, iso_components


# =====================================================================
#                   ML INFERENCE FUNCTIONS
# =====================================================================

def compute_risk_score(transaction_df, components, weights=None):
    if weights is None:
        weights = {
            "xgb": 1.0,
            "iso": 0.0,
            "graph": 0.0,
            "reputation": 0.0,
            "rules": 0.0
        }

    scores = {k: 0.0 for k in weights}

    # -------- XGBOOST --------
    if "xgb" in components:
        scaled_df = transaction_df.copy()
        scaled_df[['Amount', 'Time']] = components["xgb"]["scaler"].transform(
            transaction_df[['Amount', 'Time']]
        )
        scores["xgb"] = components["xgb"]["model"].predict_proba(
            scaled_df
        )[:, 1]

    # -------- ISOLATION FOREST --------
    if "iso" in components:
        df_iso = transaction_df.copy()
        # Ensure Feature Engineering matches Training
        df_iso["Hour"] = np.floor(df_iso["Time"] / 3600) % 24
        df_iso = df_iso.drop("Time", axis=1)

        cols_to_scale = ["Amount", "Hour"]
        df_iso[cols_to_scale] = components["iso"]["scaler"].transform(
            df_iso[cols_to_scale]
        )

        raw_scores = -components["iso"]["model"].decision_function(df_iso)
        smin = components["iso"]["score_min"]
        smax = components["iso"]["score_max"]

        # Normalize score 0-1
        scores["iso"] = np.clip(
            (raw_scores - smin) / (smax - smin + 1e-8),
            0.0,
            1.0
        )

    final_risk = sum(weights[k] * scores[k] for k in weights)
    return np.clip(final_risk, 0.0, 1.0)


def shap_explain_transaction(model, scaler, explainer, transaction_df, top_k=5):
    txn_scaled = transaction_df.copy()
    txn_scaled[['Amount', 'Time']] = scaler.transform(txn_scaled[['Amount', 'Time']])

    shap_values = explainer.shap_values(txn_scaled)[0]

    shap_df = pd.DataFrame({
        "feature": txn_scaled.columns,
        "shap_value": shap_values
    })

    shap_df["abs_shap"] = shap_df["shap_value"].abs()
    shap_df = shap_df.sort_values("abs_shap", ascending=False).head(top_k)

    explanation = [
        {"feature": row.feature, "impact": float(row.shap_value)}
        for row in shap_df.itertuples()
    ]

    return explanation