# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cb7jbzXjwbvswNLrqXa_DXGSg4HJcxWf
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, f1_score, roc_auc_score
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, recall_score
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.ensemble import IsolationForest
from sklearn.ensemble import IsolationForest
import kagglehub
import os
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import (
    average_precision_score,
    precision_recall_curve
)
import shap

# =====================================================================
#                    WRAP ENTIRE PIPELINE IN ONE FUNCTION
# =====================================================================

def run_full_ml_pipeline():
    # Download dataset from Kaggle Hub
    print("ðŸ“¥ Downloading dataset from Kaggle Hub...")
    path = kagglehub.dataset_download("mlg-ulb/creditcardfraud")
    print(f"âœ… Dataset downloaded to: {path}")

    # Display available CSVs
    csv_files = [f for f in os.listdir(path) if f.endswith('.csv')]
    print(f"ðŸ“ Available CSV files: {csv_files}")

    # Load dataset by specifying the CSV filename manually
    df = pd.read_csv(os.path.join(path, "creditcard.csv"))
    print(f"âœ… Dataset loaded successfully!")
    print(f"ðŸ“Š Shape: {df.shape}")
    df.head()

    # CHECK MISSING VALUES
    print("\nMissing Values:\n", df.isnull().sum())

    # Split dataset
    X = df.drop('Class', axis=1)
    y = df['Class']

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # TRAIN XGBOOST MODEL
    scaler = StandardScaler()
    X_train_scaled = X_train.copy()
    X_train_scaled[['Amount', 'Time']] = scaler.fit_transform(
        X_train[['Amount', 'Time']]
    )

    fraud_ratio = (y_train == 0).sum() / (y_train == 1).sum()

    model = XGBClassifier(
        max_depth=6,
        n_estimators=400,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.8,
        scale_pos_weight=fraud_ratio,
        eval_metric='aucpr',
        tree_method='hist',
        random_state=42,
        n_jobs=-1
    )

    model.fit(X_train_scaled, y_train)

    # THRESHOLD TUNING
    X_train_scaled2 = X_train.copy()
    X_train_scaled2[['Amount', 'Time']] = scaler.transform(
        X_train[['Amount', 'Time']]
    )

    y_train_prob_cv = cross_val_predict(
        model,
        X_train_scaled2,
        y_train,
        cv=3,
        method="predict_proba",
        n_jobs=1
    )[:, 1]

    precision, recall, thresholds = precision_recall_curve(
        y_train,
        y_train_prob_cv
    )

    f1_scores = 2 * (precision[:-1] * recall[:-1]) / (
        precision[:-1] + recall[:-1] + 1e-8
    )

    best_threshold = thresholds[np.argmax(f1_scores)]
    print(f"Optimal Threshold (tuned on Train): {best_threshold:.4f}")

    # TEST DATA SCALING
    X_test_scaled = X_test.copy()
    X_test_scaled[['Amount', 'Time']] = scaler.transform(
        X_test[['Amount', 'Time']]
    )

    y_test_prob = model.predict_proba(X_test_scaled)[:, 1]
    y_test_pred = (y_test_prob >= best_threshold).astype(int)

    print("\nClassification Report (Test Data):")
    print(classification_report(y_test, y_test_pred))
    print("PR-AUC:", average_precision_score(y_test, y_test_prob))

    # SHAP EXPLAINER
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X_test_scaled)

    shap.summary_plot(
        shap_values,
        X_test_scaled,
        plot_type="bar"
    )

    # Return everything needed
    return model, scaler, explainer, X_test


# =====================================================================
#                 ML INFERENCE FUNCTIONS (UNCHANGED)
# =====================================================================

def train_xgb_model(X_train, y_train):
    scaler = StandardScaler()
    X_train_scaled = X_train.copy()
    X_train_scaled[['Amount', 'Time']] = scaler.fit_transform(X_train[['Amount', 'Time']])
    fraud_ratio = (y_train == 0).sum() / (y_train == 1).sum()

    model = XGBClassifier(
        max_depth=6,
        n_estimators=400,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.8,
        scale_pos_weight=fraud_ratio,
        eval_metric='aucpr',
        tree_method='hist',
        random_state=42,
        n_jobs=-1
    )
    model.fit(X_train_scaled, y_train)
    return model, scaler


def compute_risk_score(transaction_df, components, weights=None):
    if weights is None:
        weights = {
            "xgb": 1.0,
            "iso": 0.0,
            "graph": 0.0,
            "reputation": 0.0,
            "rules": 0.0
        }

    # Initialize all channels to zero to avoid missing-key errors
    scores = {k: 0.0 for k in weights}
    if "xgb" in components:
        # Scale numeric fields the same way as training
        scaled_df = transaction_df.copy()
        scaled_df[['Amount', 'Time']] = components["xgb"]["scaler"].transform(
            transaction_df[['Amount', 'Time']]
        )
        scores["xgb"] = components["xgb"]["model"].predict_proba(
            scaled_df
        )[:, 1]
    else:
        scores["xgb"] = 0.0

    final_risk = sum(weights[k] * scores[k] for k in weights)
    return  {
    "xgb": float(scores.get("xgb", 0.0)),
    "iso": float(scores.get("iso", 0.0)),
    "final": float(np.clip(final_risk, 0.0, 1.0))
}


def shap_explain_transaction(model, scaler, explainer, transaction_df, top_k=5):
    txn_scaled = transaction_df.copy()
    txn_scaled[['Amount', 'Time']] = scaler.transform(txn_scaled[['Amount', 'Time']])

    shap_values = explainer.shap_values(txn_scaled)[0]

    shap_df = pd.DataFrame({
        "feature": txn_scaled.columns,
        "shap_value": shap_values
    })

    shap_df["abs_shap"] = shap_df["shap_value"].abs()
    shap_df = shap_df.sort_values("abs_shap", ascending=False).head(top_k)

    explanation = [
        {"feature": row.feature, "impact": float(row.shap_value)}
        for row in shap_df.itertuples()
    ]

    return explanation

