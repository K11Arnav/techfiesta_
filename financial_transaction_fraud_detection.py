# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cb7jbzXjwbvswNLrqXa_DXGSg4HJcxWf
"""

import pandas as pd
import numpy as np
try:
    import matplotlib.pyplot as plt
    import seaborn as sns
except ImportError:
    plt = None
    sns = None
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, f1_score, roc_auc_score
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, recall_score
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.ensemble import IsolationForest
from sklearn.ensemble import IsolationForest
import kagglehub
import os
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import (
    average_precision_score,
    precision_recall_curve
)
import shap
from sklearn.neighbors import NearestNeighbors

# =====================================================================
#                    WRAP ENTIRE PIPELINE IN ONE FUNCTION
# =====================================================================

def run_full_ml_pipeline():
    # Download dataset from Kaggle Hub
    print("ðŸ“¥ Downloading dataset from Kaggle Hub...")
    path = kagglehub.dataset_download("mlg-ulb/creditcardfraud")
    print(f"âœ… Dataset downloaded to: {path}")

    # Display available CSVs
    csv_files = [f for f in os.listdir(path) if f.endswith('.csv')]
    print(f"ðŸ“ Available CSV files: {csv_files}")

    # Load dataset by specifying the CSV filename manually
    df = pd.read_csv(os.path.join(path, "creditcard.csv"))
    print(f"âœ… Dataset loaded successfully!")
    print(f"ðŸ“Š Shape: {df.shape}")
    df.head()

    # CHECK MISSING VALUES
    print("\nMissing Values:\n", df.isnull().sum())

    # Split dataset
    X = df.drop('Class', axis=1)
    y = df['Class']

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # TRAIN XGBOOST MODEL
    scaler = StandardScaler()
    X_train_scaled = X_train.copy()
    X_train_scaled[['Amount', 'Time']] = scaler.fit_transform(
        X_train[['Amount', 'Time']]
    )

    fraud_ratio = (y_train == 0).sum() / (y_train == 1).sum()

    model = XGBClassifier(
        max_depth=6,
        n_estimators=400,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.8,
        scale_pos_weight=fraud_ratio,
        eval_metric='aucpr',
        tree_method='hist',
        random_state=42,
        n_jobs=-1
    )

    model.fit(X_train_scaled, y_train)

    # THRESHOLD TUNING
    X_train_scaled2 = X_train.copy()
    X_train_scaled2[['Amount', 'Time']] = scaler.transform(
        X_train[['Amount', 'Time']]
    )

    y_train_prob_cv = cross_val_predict(
        model,
        X_train_scaled2,
        y_train,
        cv=3,
        method="predict_proba",
        n_jobs=1
    )[:, 1]

    precision, recall, thresholds = precision_recall_curve(
        y_train,
        y_train_prob_cv
    )

    f1_scores = 2 * (precision[:-1] * recall[:-1]) / (
        precision[:-1] + recall[:-1] + 1e-8
    )

    best_threshold = thresholds[np.argmax(f1_scores)]
    print(f"Optimal Threshold (tuned on Train): {best_threshold:.4f}")

    # TEST DATA SCALING
    X_test_scaled = X_test.copy()
    X_test_scaled[['Amount', 'Time']] = scaler.transform(
        X_test[['Amount', 'Time']]
    )

    y_test_prob = model.predict_proba(X_test_scaled)[:, 1]
    y_test_pred = (y_test_prob >= best_threshold).astype(int)

    print("\nClassification Report (Test Data):")
    print(classification_report(y_test, y_test_pred))
    print("PR-AUC:", average_precision_score(y_test, y_test_prob))

    # SHAP EXPLAINER
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X_test_scaled)

    if sns and plt:
        shap.summary_plot(
            shap_values,
            X_test_scaled,
            plot_type="bar"
        )

    # =================================================================
    #                   ISOLATION FOREST SECTION
    # =================================================================
    print("\n" + "="*40)
    print("    STARTING ISOLATION FOREST TRAINING")
    print("="*40)

    # 1. Feature Engineering (Specific to ISO)
    X_train_iso = X_train.copy()
    X_train_iso['Hour'] = X_train_iso['Time'].apply(lambda x: np.floor(x / 3600) % 24)
    X_train_iso = X_train_iso.drop('Time', axis=1)

    # 2. Scaling
    iso_scaler = StandardScaler()
    cols_to_scale = ['Amount', 'Hour']
    X_train_iso[cols_to_scale] = iso_scaler.fit_transform(X_train_iso[cols_to_scale])

    # 3. Model Training
    contamination = (y_train == 1).sum() / len(y_train)
    iso_model = IsolationForest(
        n_estimators=100,
        contamination=max(contamination, 0.001),
        random_state=42,
        n_jobs=-1
    )
    iso_model.fit(X_train_iso)

    # 4. Score normalization parameters
    y_train_scores_iso = iso_model.decision_function(X_train_iso)
    iso_components = {
        "model": iso_model,
        "scaler": iso_scaler,
        "score_min": float(y_train_scores_iso.min()),
        "score_max": float(y_train_scores_iso.max())
    }

    # =================================================================
    #                   GRAPH / SIMILARITY SECTION
    # =================================================================
    print("\n" + "="*40)
    print("    STARTING GRAPH NEIGHBORHOOD TRAINING")
    print("="*40)

    # For the graph, we use V1-V28 features
    v_cols = [f'V{i}' for i in range(1, 29)]
    X_train_v = X_train[v_cols].values

    # Fit KNN on a sample for performance if needed, but here we can use a reasonable subset
    # Let's take 20k samples including ALL fraud to ensure we have fraud neighbors
    fraud_indices = y_train[y_train == 1].index
    non_fraud_indices = y_train[y_train == 0].sample(min(20000, sum(y_train == 0)), random_state=42).index
    reference_indices = fraud_indices.union(non_fraud_indices)

    X_ref = X_train.loc[reference_indices]
    y_ref = y_train.loc[reference_indices]

    knn = NearestNeighbors(n_neighbors=10, metric='euclidean', n_jobs=-1)
    knn.fit(X_ref[v_cols])

    graph_components = {
        "model": knn,
        "reference_data": {
            "features": X_ref[v_cols].values.tolist(),
            "labels": y_ref.values.tolist(),
            "amounts": X_ref['Amount'].values.tolist(),
            "v_cols": v_cols
        }
    }

    # Return everything needed
    return model, scaler, explainer, X_test, iso_components, graph_components


# =====================================================================
#                 ML INFERENCE FUNCTIONS (UNCHANGED)
# =====================================================================

def train_xgb_model(X_train, y_train):
    scaler = StandardScaler()
    X_train_scaled = X_train.copy()
    X_train_scaled[['Amount', 'Time']] = scaler.fit_transform(X_train[['Amount', 'Time']])
    fraud_ratio = (y_train == 0).sum() / (y_train == 1).sum()

    model = XGBClassifier(
        max_depth=6,
        n_estimators=400,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.8,
        scale_pos_weight=fraud_ratio,
        eval_metric='aucpr',
        tree_method='hist',
        random_state=42,
        n_jobs=-1
    )
    model.fit(X_train_scaled, y_train)
    return model, scaler


def compute_risk_score(transaction_df, components, weights=None):
    if weights is None:
        weights = {
            "xgb": 0.6,
            "iso": 0.2,
            "graph": 0.2
        }

    scores = {k: 0.0 for k in weights}

    # -------- XGBOOST --------
    if "xgb" in components:
        scaled_df = transaction_df.copy()
        scaled_df[['Amount', 'Time']] = components["xgb"]["scaler"].transform(
            transaction_df[['Amount', 'Time']]
        )
        scores["xgb"] = float(components["xgb"]["model"].predict_proba(scaled_df)[:, 1][0])

    # -------- ISOLATION FOREST --------
    if "iso" in components:
        df_iso = transaction_df.copy()
        df_iso["Hour"] = np.floor(df_iso["Time"] / 3600) % 24
        df_iso = df_iso.drop("Time", axis=1)

        cols_to_scale = ["Amount", "Hour"]
        df_iso[cols_to_scale] = components["iso"]["scaler"].transform(df_iso[cols_to_scale])

        raw_score = -components["iso"]["model"].decision_function(df_iso)[0]
        smin = components["iso"]["score_min"]
        smax = components["iso"]["score_max"]

        scores["iso"] = float(np.clip((raw_score - smin) / (smax - smin + 1e-8), 0.0, 1.0))

    # -------- GRAPH / KNN --------
    if "graph" in components:
        v_cols = [f'V{i}' for i in range(1, 29)]
        query_v = transaction_df[v_cols].values
        
        distances, indices = components["graph"]["model"].kneighbors(query_v)
        
        # 1. Fraud Density
        neighbor_labels = np.array(components["graph"]["reference_data"]["labels"])[indices[0]]
        fraud_density = np.mean(neighbor_labels)
        
        # 2. Local Anomaly (Relative Distance)
        # We can use the distance to the 10th neighbor as a simple density measure
        avg_dist = np.mean(distances[0])
        # Simple normalization (this is heuristic)
        local_iso = np.clip(avg_dist / 10.0, 0.0, 1.0)
        
        # 3. Amount Deviation
        neighbor_amounts = np.array(components["graph"]["reference_data"]["amounts"])[indices[0]]
        median_amt = np.median(neighbor_amounts)
        std_amt = np.std(neighbor_amounts) + 1e-8
        amt_deviation = np.clip(abs(transaction_df['Amount'].values[0] - median_amt) / (3 * std_amt), 0.0, 1.0)
        
        # Combine into graph score
        scores["graph"] = float(0.5 * fraud_density + 0.3 * local_iso + 0.2 * amt_deviation)

    final_risk = sum(weights.get(k, 0) * scores.get(k, 0) for k in scores)
    
    # Also return neighboring info for frontend "Click-to-Explain"
    neighbor_info = []
    if "graph" in components:
        for idx in indices[0]:
            neighbor_info.append({
                "label": int(components["graph"]["reference_data"]["labels"][idx]),
                "amount": float(components["graph"]["reference_data"]["amounts"][idx])
            })

    return {
        "xgb": scores.get("xgb", 0.0),
        "iso": scores.get("iso", 0.0),
        "graph": scores.get("graph", 0.0),
        "final": float(np.clip(final_risk, 0.0, 1.0)),
        "neighbors": neighbor_info
    }


def shap_explain_transaction(model, scaler, explainer, transaction_df, top_k=5):
    txn_scaled = transaction_df.copy()
    txn_scaled[['Amount', 'Time']] = scaler.transform(txn_scaled[['Amount', 'Time']])

    shap_values = explainer.shap_values(txn_scaled)[0]

    shap_df = pd.DataFrame({
        "feature": txn_scaled.columns,
        "shap_value": shap_values
    })

    shap_df["abs_shap"] = shap_df["shap_value"].abs()
    shap_df = shap_df.sort_values("abs_shap", ascending=False).head(top_k)

    explanation = [
        {"feature": row.feature, "impact": float(row.shap_value)}
        for row in shap_df.itertuples()
    ]

    return explanation

