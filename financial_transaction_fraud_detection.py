# -*- coding: utf-8 -*-
"""Financial transaction fraud detection

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/namitava/financial-transaction-fraud-detection.73cbc90c-d991-43d8-94a0-96d93dac49ec.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251213/auto/storage/goog4_request%26X-Goog-Date%3D20251213T153204Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D639c2d9c443d3898a89357b1b9bf8c4aec51ea3962424ea917baac307d99e0d38ba51079ac87c67f266759111e12ba0e639d31481354755fd462184d228fdb756650118b324df34134cf1e77f0c4b1d98d1fb5b586c448765a99ab4a879d2de53be80b4fd9c3257c1ac2d01639089fd15914d7259f89213743e15c0ae3816b64bac88043302e51918c08b2b5f77701f4fac5acdfbe70eb8939bae82a0d2cbf9bc3c3a8a1f0eca7ef9c41d9b51e6007959faee2a4aaf52c1484c6941c2e893738d5a0b41b86aea8fdf7dcc0deafa3ae4083c4a232d3f8bd3b6e58ba0710b571e77ec9db00e395a76ec557274d52005a1dff507b8133df4d9490f9f6b0c25c3a45

# DATA PREPARATION
Install data and assign csv data file into pandas Dataframe
"""

import kagglehub
import pandas as pd
import numpy as np
import os
import warnings
warnings.filterwarnings("ignore")

# Download dataset
print("ðŸ“¥ Downloading dataset from Kaggle Hub...")
path = kagglehub.dataset_download("aryan208/financial-transactions-dataset-for-fraud-detection")
print(f"âœ… Dataset downloaded to: {path}")

csv_files = [f for f in os.listdir(path) if f.endswith('.csv')]
print(f"ðŸ“ Available CSV files: {csv_files}")

# Load dataset
df = pd.read_csv(os.path.join(path, "financial_fraud_detection_dataset.csv"))
print(f"âœ… Dataset loaded successfully!")

import seaborn as sns
import polars as pl
import matplotlib.pyplot as plt
from scipy import stats
import warnings
warnings.filterwarnings("ignore")

"""# DATA EXPLORATION (EDA)"""

#The first 5 data from the dataset
df.head()

"""## Data Overview
As we can see, the label for this dataset is attribute is_fraud with boolean label (True/False) to determine whether a transaction is fraudulent or legitimate
"""

#Display basic info of the dataset
print("Display Info: \n")
print(df.info())

#Check for missing value
print(f"Shape of dataset: {df.shape}")
print(f"The amount of missing value: {df.isnull().sum()}")

#Class distribution
fraud_trans_count = df["is_fraud"] == True
legit_trans_count = df["is_fraud"] == False
print(f"Fraud transactions: {fraud_trans_count.sum()} ({fraud_trans_count.sum()*100/df.shape[0]:.2f}%)")
print(f"Legitimate transactions: {legit_trans_count.sum()} ({legit_trans_count.sum()*100/df.shape[0]:.2f}%)")

plt.figure(figsize=(8,7))
ax = sns.countplot(x='is_fraud',data=df,palette='coolwarm')
plt.title("Fraudulent Financial Transaction Distribution")
plt.xlabel("Fraud Check")
plt.ylabel("Count")

for p in ax.patches:
    ax.annotate(f'{int(p.get_height())}',
                (p.get_x() + p.get_width() / 2., p.get_height()),
                ha = 'center', va = 'center',
                fontsize=11, color='black',
                xytext=(0, 6),textcoords='offset points')
plt.show()

df["timestamp"] = pd.to_datetime(df["timestamp"], errors="coerce")
df["hour"] = df["timestamp"].dt.hour
df["day"] = df["timestamp"].dt.day
df["day_of_week"] = df["timestamp"].dt.weekday
df["month"] = df["timestamp"].dt.month

"""## Convert Boolean labels into Numeric labels (0 and 1)
Models such as SVM, XGBoost,... prefer 0 and 1 more than True and False so I convert it
"""

df["is_fraud"] = df["is_fraud"].astype(int)
df.head()

df_pandas = df.sample(n=df.shape[0],random_state = 19)
plt.figure(figsize=(10, 6))  # set figure size first
sns.countplot(x="month", data=df, palette="coolwarm", hue="is_fraud")
plt.title("Fraud Distribution Over Time")
plt.xlabel("Month")
plt.ylabel("Count")
plt.show()

print(f"Statistic Describe: \n")
df.describe()

"""## Statistical Summary
1. *Amount*: range from 0.01 to 3520.57, mean â‰ˆ358.9343, highly skewed (â‰ˆ469.93). Amount of money in a transaction may affect the fraudulent rate of itself  
2. *spending_deviation_score*: range from -5.2 to 5.02, mean â‰ˆ0, std â‰ˆ1. Like normal distribution, useful for fraud detection  
3. *velocity_score*: range from 1 to 20, mean â‰ˆ10.5, std â‰ˆ5.77. High value may indicate rapid transactions â€“ sign of fraud  
4. *geo_anomaly_score*: range from 0 to 1, mean â‰ˆ0.5, std â‰ˆ0.3. Likely a probability-like score, critical for fraud analysis.
5. *time_since_last_transaction*: range from -8777.814 to 8757.758, mean â‰ˆ1.53, std â‰ˆ3576.569. Highly skewed so it may not be effective for detecting fraud transaction
"""

### Correlation for numeric variables
corr_df = df[["timestamp","amount","time_since_last_transaction","spending_deviation_score","velocity_score","geo_anomaly_score","day","month","is_fraud"]]
correlation_matrix = corr_df.corr()
plt.figure(figsize=(10,6))
sns.heatmap(correlation_matrix,annot=True,cmap='coolwarm',center=0,linewidths=0.5,cbar=True,fmt='.2f')
plt.title("Correlation between numeric variables",fontsize=16)
plt.show()

"""## Timestamp
Here we only consider month and day ofr fraud detection
"""

### Month
fraud_by_month = (
    df.groupby("month")
    .agg(
        total_trans=("is_fraud", "count"),
        fraud_trans=("is_fraud", "sum")
    )
)
fraud_by_month["fraud_rate (%)"] = fraud_by_month["fraud_trans"]*100 / fraud_by_month["total_trans"]
fraud_by_month["fraud_rate (%)"] = fraud_by_month["fraud_rate (%)"].round(2)
fraud_by_month.sort_values("fraud_rate (%)",ascending=False).head(10)

plt.figure(figsize=(10,6))
ax = sns.countplot(x='month',hue='is_fraud',data=df,palette='coolwarm')
plt.title('Month for Fraud Detection')
plt.legend(title='is_fraud',loc='center right')
plt.xlabel('Month')
plt.ylabel('Count')
for p in ax.patches:
    ax.annotate(f'{int(p.get_height())}',
                (p.get_x() + p.get_width() / 2., p.get_height()),
                ha = 'center', va = 'center',
                fontsize=8, color='black',
                xytext=(0, 6),textcoords='offset points')
plt.show()

### Day
fraud_by_day = (
    df.groupby("day")
    .agg(
        total_trans=("is_fraud", "count"),
        fraud_trans=("is_fraud", "sum")
    )
)
fraud_by_day["fraud_rate (%)"] = fraud_by_day["fraud_trans"]*100 / fraud_by_day["total_trans"]
fraud_by_day["fraud_rate (%)"] = fraud_by_day["fraud_rate (%)"].round(2)
fraud_by_day.sort_values("fraud_rate (%)",ascending=False).head(100)

plt.figure(figsize=(22,6))
ax = sns.countplot(x='day',hue='is_fraud',data=df,palette='coolwarm')
plt.title('Day for Fraud Detection')
# plt.legend(title='is_fraud',loc='center right')
plt.xlabel('Day in month')
plt.ylabel('Count')
for p in ax.patches:
    ax.annotate(f'{int(p.get_height())}',
                (p.get_x() + p.get_width() / 2., p.get_height()),
                ha = 'center', va = 'center',
                fontsize=8, color='black',
                xytext=(0, 6),textcoords='offset points')
plt.show()

"""## Sender/Receiver Account"""

# Sender/Receiver
print(f"Number of senders: {np.unique(df['sender_account']).size}")
print(f"Number of receivers: {np.unique(df['receiver_account']).size}")

# Top 10 most active sender in the dataset
df.groupby("sender_account").size().reset_index(name="count").sort_values("count", ascending=False).head(10)

# Top 10 most active receiver in the dataset
df.groupby("receiver_account").size().reset_index(name="count").sort_values("count",ascending=False).head(10)

# Calculate the rate that related to each account
fraud_by_sender = (
    df.groupby("sender_account")
    .agg(
        total_trans=("is_fraud", "count"),
        fraud_trans=("is_fraud", "sum")
    )
)
fraud_by_sender["fraud_rate (%)"] = fraud_by_sender["fraud_trans"]*100 / fraud_by_sender["total_trans"]
fraud_by_sender["fraud_rate (%)"] = fraud_by_sender["fraud_rate (%)"].round(2)
fraud_by_sender.sort_values("fraud_rate (%)",ascending=False).head(10)

fraud_by_receiver = (
    df.groupby("receiver_account").agg(
        total_trans=('is_fraud','count'),
        fraud_trans=('is_fraud','sum')
    )
)
fraud_by_receiver['fraud_rate (%)'] = fraud_by_receiver['fraud_trans']*100/fraud_by_receiver['total_trans']
fraud_by_receiver['fraud_rate (%)'] = fraud_by_receiver['fraud_rate (%)'].round(2)
fraud_by_receiver.sort_values("fraud_rate (%)",ascending=False).head(10)

#### Top 10 most active pair of accounts in the dataset

df.groupby(["sender_account","receiver_account"]).agg(
    total_trans=('is_fraud','count'),
    fraud_trans=('is_fraud','sum')
).sort_values("fraud_trans",ascending=False).head(10)

"""## Amount"""

sns.histplot(df,x='amount',hue='is_fraud',bins=100,kde=True,log_scale=True)
plt.xlabel('Amount of a transaction')
plt.title("Transaction amount by fraud (log scale)")
plt.show()

"""## Merchant Category"""

fraud_by_mc = (df.groupby('merchant_category').agg(
    total_trans=('is_fraud','count'),
    fraud_trans=('is_fraud','sum')
))
fraud_by_mc['fraud_rate (%)'] = fraud_by_mc['fraud_trans']*100/fraud_by_mc['total_trans']
fraud_by_mc['fraud_rate (%)']= fraud_by_mc['fraud_rate (%)'].round(2)
fraud_by_mc.sort_values("fraud_rate (%)",ascending=False).head(10)

plt.figure(figsize=(10,6))
ax = sns.countplot(x='merchant_category',hue='is_fraud',data=df,palette='coolwarm')
plt.title('Merchant Category Fraud Detection')
plt.xlabel('Merchant Category')
plt.ylabel('Count')
for p in ax.patches:
    ax.annotate(f'{int(p.get_height())}',
                (p.get_x() + p.get_width() / 2., p.get_height()),
                ha = 'center', va = 'center',
                fontsize=8, color='black',
                xytext=(0, 6),textcoords='offset points')
plt.show()

"""## Transaction Type"""

df.groupby("transaction_type").size().reset_index(name="Count").sort_values("Count",ascending=False).head(10)

#Lets see what fraud most use transaction type
fraud_by_tp = (df.groupby('transaction_type').agg(
    total_trans=('is_fraud','count'),
    fraud_trans=('is_fraud','sum')
))
fraud_by_tp['fraud_rate (%)'] = fraud_by_tp['fraud_trans']*100/fraud_by_tp['total_trans']
fraud_by_tp['fraud_rate (%)']= fraud_by_tp['fraud_rate (%)'].round(2)
fraud_by_tp.sort_values("fraud_rate (%)",ascending=False).head()

plt.figure(figsize=(10,6))
ax = sns.countplot(x='transaction_type',hue='is_fraud',data=df,palette='coolwarm')
plt.title('Fraud vs Legit transaction type amount')
plt.xlabel('Transaction type')
plt.ylabel('Count')
for p in ax.patches:
    ax.annotate(f'{int(p.get_height())}',
                (p.get_x() + p.get_width() / 2., p.get_height()),
                ha = 'center', va = 'center',
                fontsize=8, color='black',
                xytext=(0, 6),textcoords='offset points')
plt.show()

"""## Spending Deviation Score"""

fraud_by_sds = (df.groupby("spending_deviation_score").agg(
    total_trans=('is_fraud','count'),
    fraud_trans=('is_fraud','sum')
).sort_values('fraud_trans',ascending=False))
fraud_by_sds['fraud_rate (%)'] = fraud_by_sds['fraud_trans']*100/fraud_by_sds['total_trans']
fraud_by_sds['fraud_rate (%)'] = fraud_by_sds['fraud_rate (%)'].round(2)
fraud_by_sds.head(10)

plt.figure(figsize=(10,6))
sns.histplot(data=df,x='spending_deviation_score',hue='is_fraud',bins=100,kde=True)
plt.title("Spending Deviation Score Fraud Detection")
plt.show()

"""## Velocity Score"""

fraud_by_vs = (df.groupby("velocity_score").agg(
    total_trans=('is_fraud','count'),
    fraud_trans=('is_fraud','sum')
))
fraud_by_vs['fraud_rate (%)'] = fraud_by_vs['fraud_trans']*100/fraud_by_vs['total_trans']
fraud_by_vs['fraud_rate (%)'] = fraud_by_vs['fraud_rate (%)'].round(2)
fraud_by_vs.sort_values('fraud_rate (%)',ascending=False).head(10)

plt.figure(figsize=(17,7))
ax = sns.countplot(data=df,x='velocity_score',hue='is_fraud',palette='coolwarm')
ax.legend(title="is_fraud",loc='center right')
plt.title("Velocity Score Fraud Detection")
plt.xlabel("Velocity Score")
plt.ylabel("Count")
for p in ax.patches:
    ax.annotate(f'{int(p.get_height())}',
                (p.get_x() + p.get_width() / 2., p.get_height()),
                ha = 'center', va = 'center',
                fontsize=8, color='black',
                xytext=(0, 6),textcoords='offset points')
plt.show()

"""## Geo Anomaly Score"""

fraud_by_gas = (df.groupby("geo_anomaly_score").agg(
    total_trans=('is_fraud','count'),
    fraud_trans=('is_fraud','sum')
))
fraud_by_gas['fraud_rate (%)'] = fraud_by_gas['fraud_trans']*100/fraud_by_gas['total_trans']
fraud_by_gas['fraud_rate (%)'] = fraud_by_gas['fraud_rate (%)'].round(2)
fraud_by_gas.sort_values('fraud_rate (%)').head(10)

plt.figure(figsize=(10,6))
sns.histplot(data=df,x='geo_anomaly_score',bins=20,hue='is_fraud',kde=True)
plt.title("Geo Anomaly Score Fraud Detection")
plt.xlabel("Geo Anomaly Score")
plt.ylabel("Count")
plt.show()

"""## IP Address"""

fraud_by_ip = (df.groupby("ip_address").agg(
    total_trans=('is_fraud','count'),
    fraud_trans=('is_fraud','sum')
))
fraud_by_ip['fraud_rate (%)'] = fraud_by_ip['fraud_trans']*100/fraud_by_ip['total_trans']
fraud_by_ip['fraud_rate (%)'] = fraud_by_ip['fraud_rate (%)'].round(2)
fraud_by_ip.sort_values('fraud_rate (%)',ascending=False).head(10)

"""## Location"""

fraud_by_loc = (df.groupby("location").agg(
    total_trans=('is_fraud','count'),
    fraud_trans=('is_fraud','sum')
))
fraud_by_loc['fraud_rate (%)'] = fraud_by_loc['fraud_trans']*100/fraud_by_loc['total_trans']
fraud_by_loc['fraud_rate (%)'] = fraud_by_loc['fraud_rate (%)'].round(2)
fraud_by_loc.sort_values('fraud_rate (%)',ascending=False).head(10)

plt.figure(figsize=(10,6))
ax = sns.countplot(x='location',hue='is_fraud',data=df,palette='coolwarm')
plt.title('Location Fraud Detection')
plt.xlabel('Location')
plt.ylabel('Count')
plt.legend(title='is_fraud',loc='center right')
for p in ax.patches:
    ax.annotate(f'{int(p.get_height())}',
                (p.get_x() + p.get_width() / 2., p.get_height()),
                ha = 'center', va = 'center',
                fontsize=8, color='black',
                xytext=(0, 6),textcoords='offset points')
plt.show()

"""## Payment Channel"""

fraud_by_pc = (df.groupby("payment_channel").agg(
    total_trans=('is_fraud','count'),
    fraud_trans=('is_fraud','sum')
))
fraud_by_pc['fraud_rate (%)'] = fraud_by_pc['fraud_trans']*100/fraud_by_pc['total_trans']
fraud_by_pc['fraud_rate (%)'] = fraud_by_pc['fraud_rate (%)'].round(2)
fraud_by_pc.sort_values('fraud_rate (%)',ascending=False).head(10)

plt.figure(figsize=(10,6))
ax = sns.countplot(x='payment_channel',hue='is_fraud',data=df,palette='coolwarm')
plt.title('Payment Channel Fraud Detection')
plt.xlabel('Payment Channel')
plt.ylabel('Count')
plt.legend(title='is_fraud',loc='center right')
for p in ax.patches:
    ax.annotate(f'{int(p.get_height())}',
                (p.get_x() + p.get_width() / 2., p.get_height()),
                ha = 'center', va = 'center',
                fontsize=8, color='black',
                xytext=(0, 6),textcoords='offset points')
plt.show()

"""## Device Hash"""

fraud_by_dh = (df.groupby("device_hash").agg(
    total_trans=('is_fraud','count'),
    fraud_trans=('is_fraud','sum')
))
fraud_by_dh['fraud_rate (%)'] = fraud_by_dh['fraud_trans']*100/fraud_by_dh['total_trans']
fraud_by_dh['fraud_rate (%)'] = fraud_by_dh['fraud_rate (%)'].round(2)
fraud_by_dh.sort_values('fraud_rate (%)',ascending=False).head(10)

"""# DATA PREPROCESSING
1. Data Cleaning/Preprocessing
   Method:
    - SimpleImpute (Handle missing value)
    - LabelEncoder (Handle Categorical Features)
    - Scaler (Hanedle Numeric Features)
2. Feature Engineering
3. Feature Selection/Dimension Reduction
"""

## Library for Data Processing
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA

#1 Handle Missing values
### time_since_last_spend
imputer = SimpleImputer(strategy="mean")
df[df.select_dtypes(include=np.number).columns] = imputer.fit_transform(
    df.select_dtypes(include=np.number)
)
df.head()

### fraud_type
## Because fraud type does not mean anything else than is_fraud so we drop it
df = df.drop(columns=["fraud_type"])
df.head()

#2 Handle categorical features
for col in df.select_dtypes(include=["object","category"]).columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col].astype(str))
df.head()

"""# FEATURE ENGINEERING
1. Scaler (scaling data)
2. Data creation
"""

## Amount features
df["amount_per_velocity"] = df["amount"]/(df["velocity_score"] + 1)
df["amount_log"] = np.log1p(df["amount"])
df["amount_to_avg_ratio"] = df["amount"]/df.groupby("sender_account")["amount"].transform("mean")
## Frequency features
df["transaction_per_day"] = df.groupby(["sender_account","day"])["amount"].transform("count")
df["transaction_gap"] = (df.groupby("sender_account")["timestamp"].diff().dt.total_seconds().fillna(0))
## Risk features
df["is_night_transaction"] = df["hour"].between(18,24).astype(int)
df["is_weekend"] = df["day_of_week"].isin([6,8]).astype(int)
df["is_self_transfer"] = (df["sender_account"] == df["receiver_account"]).astype(int)
## Network features
df["sender_degree"] = df.groupby("sender_account")["receiver_account"].transform("nunique")
df["receiver_degree"] = df.groupby("receiver_account")["sender_account"].transform("nunique")
df["sender_total_transaction"] = df.groupby("sender_account")["amount"].transform("count")
df["receiver_total_transaction"] = df.groupby("receiver_account")["amount"].transform("count")
## Aggregation features
df["sender_avg_amount"] = df.groupby("sender_account")["amount"].transform("mean")
df["sender_std_amount"] = df.groupby("sender_account")["amount"].transform("std").fillna(0)
## Fraud features
df["sender_fraud_transaction"] = df.groupby("sender_account")["is_fraud"].transform("sum")
df["receiver_fraud_transaction"] = df.groupby("receiver_account")["is_fraud"].transform("sum")

df["sender_fraud_percentage (%)"] = (df["sender_fraud_transaction"]*100/df["sender_total_transaction"]).round(2)
df["receiver_fraud_percentage (%)"] = (df["receiver_fraud_transaction"]*100/df["receiver_total_transaction"]).round(2)

df[["sender_fraud_percentage (%)", "receiver_fraud_percentage (%)"]] = df[["sender_fraud_percentage (%)", "receiver_fraud_percentage (%)"]].fillna(0)
## Others
df["deviation_squared"] = df["spending_deviation_score"] ** 2
df.head(10)

correlation_matrix = df.corr()
plt.figure(figsize=(10,6))
sns.heatmap(correlation_matrix,annot=False,cmap='coolwarm',center=0,linewidths=0.5,cbar=True)
plt.title("Correlation between numeric variables",fontsize=16)
plt.show()

# import os

# dest = os.getcwd() + '/dataset'
# os.makedirs(
#     dest, exist_ok=True
# )

# df.to_csv(dest + '/financial_fraud_detection_dataset.csv')

"""## Train, Test, Validation Split"""

# import os
# import pandas as pd

# dest = os.getcwd() + '/dataset/financial_fraud_detection_dataset.csv'
# df = pd.read_csv(dest)
# df.describe()

# df.drop(columns=['Unnamed: 0', 'timestamp'], inplace=True)
df.drop(columns=['timestamp'], inplace=True)

df_majority = df[df['is_fraud'] == 0]
df_minority = df[df['is_fraud'] == 1]

df_majority_downsampled = df_majority.sample(n=2*len(df_minority), random_state=36)
df_balanced = pd.concat([df_majority_downsampled, df_minority])
df_balanced.shape

from sklearn.model_selection import train_test_split

y = df_balanced['is_fraud']
X = df_balanced.drop(columns=['is_fraud'])


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=36, stratify=y)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=1/8, random_state=36, stratify=y_train)
X_train.head()

len(X_train), len(y_train), len(X_test), len(y_test)

X_train.shape, X_test.shape, X_val.shape

import sklearn
print(sklearn.__version__)

"""## Train XGBoost model"""

from xgboost import XGBClassifier

scale_pos_weight = y_train[y_train==1].count() / y_train[y_train==0].count()
xgb_model = XGBClassifier(n_estimators=300,
                        objective='binary:logistic',
                        tree_method='hist',
                        max_depth=12,
                        learning_rate=0.05,
                        reg_lambda=3.6,
                        reg_alpha=3.6,
                        scale_pos_weight=scale_pos_weight,
                        eval_metric=['aucpr'],
                        verbosity=2,
                        subsample=0.8,
                        device='cuda',
                        n_jobs=-1)

print("Training model ...")
eval_set=[(X_train, y_train),(X_val, y_val)]
xgb_model.fit(X_train, y_train,
             eval_set=eval_set,
             verbose=50)
print("Training complete!")

from sklearn.metrics import precision_recall_curve
import matplotlib.pyplot as plt

y_pred_proba = xgb_model.predict_proba(X_test)[:, 1]
precisions, recalls, thresholds = precision_recall_curve(y_test, y_pred_proba)

plt.figure(figsize=(10, 6))
plt.plot(thresholds, precisions[:-1], "b--", label="Precision")
plt.plot(thresholds, recalls[:-1], "g-", label="Recall")
plt.xlabel('Threshold')
plt.legend(loc='center left')
plt.title('Precision and Recall vs. Threshold')
plt.show()

from sklearn.metrics import classification_report, confusion_matrix, average_precision_score

#Evaluate
y_pred_proba = xgb_model.predict_proba(X_test)[: , 1]
print(classification_report(y_test, (y_pred_proba > 0.5).astype(int)))
print(f"AP: {average_precision_score(y_test, y_pred_proba):.4f}")

from sklearn.metrics import roc_curve, roc_auc_score


# Calculate AUC-ROC
auc_score = roc_auc_score(y_test, y_pred_proba)
print(f"AUC-ROC: {auc_score:.4f}")

# Plot the curve
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
plt.plot(fpr, tpr, label=f"AUC = {auc_score:.2f}")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate (Recall)')
plt.legend()
plt.show()

plt.figure(figsize=(60,36))
plt.bar(X.columns.tolist(), xgb_model.feature_importances_)
plt.show()

"""## Train IsolationForest model"""

from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import RobustScaler

scaler = RobustScaler()
scaler.fit(X_train)
X_train, X_test = scaler.transform(X_train), scaler.transform(X_test)

iso_model = IsolationForest(
    n_estimators=300,
    max_samples=128,
    max_features=3,
    contamination=y_train.mean(),
    bootstrap=False,
    verbose=2,
    random_state=36,
    n_jobs=-1
)

print("Training...")
iso_model.fit(X_train)
print("Training completed!")

y_pred_iso = iso_model.predict(X_test)
print(np.unique(y_pred_iso, return_counts=True))
y_pred_iso = np.where(y_pred_iso == -1, 1, 0)

print(classification_report(y_test, y_pred_iso))
print(f"AP: {average_precision_score(y_test, y_pred_iso):.4f}")

from sklearn.metrics import precision_recall_curve, average_precision_score, auc
score = iso_model.decision_function(X_test)
ap_score = average_precision_score(y_test, -score)
print(f"Average Precision: {ap_score:.4f}")

precision, recall, _ = precision_recall_curve(y_test, -score)
auprc = auc(recall, precision)
print(f"AUPRC: {auprc:.4f}")

from sklearn.metrics import roc_curve, roc_auc_score

# Calculate AUC-ROC
auc_score = roc_auc_score(y_test, -score)
print(f"AUC-ROC: {auc_score:.4f}")

# Plot the curve
fpr, tpr, _ = roc_curve(y_test, -score)
plt.plot(fpr, tpr, label=f"AUC = {auc_score:.2f}")
plt.plot([0,1], [0,1], color='navy', lw=2, linestyle='--', label='Random Guessing')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate (Recall)')
plt.legend()
plt.show()